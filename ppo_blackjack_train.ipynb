{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import ppo_blackjack_agent as a\n",
    "import ppo_blackjack_train as t\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path().cwd()/'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_args = {\n",
    "    \"exp_name\": \"ex1\",\n",
    "    \"seed\": 2025,\n",
    "    \"env_name\": \"Blackjack-v1\",\n",
    "    \"model_name\": \"Blackjack-v1\",\n",
    "    \"max_episode_steps\": 100, # not relevant for Blackjack\n",
    "    \"train_episodes\": 500000,\n",
    "    \"batch_size\": 256,\n",
    "    \"min_update_samples\": 5000,\n",
    "    \"testing\": False,\n",
    "    \"model_path\": \"default\",\n",
    "    \"save_video\": True,\n",
    "    \"save_model\": True,\n",
    "    \"save_logging\": True,\n",
    "    \"silent\": False,\n",
    "    \"use_wandb\": True,\n",
    "    \"run_suffix\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.train(cfg_args=cfg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    u.plot_reward(Path().cwd()/'results'/'logging'/'Blackjack-v1_2025.csv', 'Blackjack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = work_dir / 'model' / f'{cfg_args[\"model_name\"]}_params.pt'\n",
    "\n",
    "observation_space_dim = 3\n",
    "action_space_dim = 2\n",
    "policy = a.Policy(observation_space_dim, action_space_dim)\n",
    "\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "state_dict = torch.load(model_path, map_location='cuda')\n",
    "policy.load_state_dict(state_dict)\n",
    "\n",
    "policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "value_grid_usable_ace, policy_grid_usable_ace = u.create_grids_nn(policy, usable_ace=True)\n",
    "fig1 = u.create_plots(value_grid_usable_ace, policy_grid_usable_ace, title=\"With usable ace\")\n",
    "plt.show()\n",
    "\n",
    "value_grid_no_ace, policy_grid_no_ace = u.create_grids_nn(policy, usable_ace=False)\n",
    "fig2 = u.create_plots(value_grid_no_ace, policy_grid_no_ace, title=\"Without usable ace\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6204c3",
   "metadata": {},
   "source": [
    "Overall it seems to converge to a reasonable policy, though not the optimal one. The differences are quite small and for edge cases longer training might still change some of the decisions.\n",
    "\n",
    "A good benchmark for the problem can be found here: https://chisness.github.io/2020-09-21/monte-carlo-rl-and-blackjack\n",
    "\n",
    "Their optimal policy, found with a Monte Carlo method after 10 million iterations is almost identical (there is a single difference) to the optimal policy proposed by Sutton and Barto in their RL book.\n",
    "\n",
    "The method also seems better suitable for blackjack than PPO algorithm, but the point was to test that the implementation works for blackjack and based on this it does. For the absolute optimal policy hyperparameters, batch size and episode amount could be adjusted. With this policy the reward is about -0.050 when the optimal policy yields -0.047."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
